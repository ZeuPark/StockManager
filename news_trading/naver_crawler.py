"""
Naver News Crawler
ì…€ë ˆë‹ˆì›€ ê¸°ë°˜ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
"""

import time
import pandas as pd
from tqdm import tqdm
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
from .config import NEWS_SOURCES

logger = logging.getLogger(__name__)

class NaverNewsCrawler:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless: bool = True):
        self.headless = headless
        self.driver = None
        self.setup_driver()
    
    def setup_driver(self):
        """ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì„¤ì •"""
        try:
            options = Options()
            if self.headless:
                options.add_argument("--headless")
            options.add_argument("--disable-gpu")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            self.driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()), 
                options=options
            )
            logger.info("ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def extract_detail_text(self, url: str) -> str:
        """ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            self.driver.get(url)
            time.sleep(0.5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì„ íƒìë“¤
            content_selectors = [
                'div#articleBodyContents',
                'div#articleBody',
                'div.article_body',
                'div.article_body_wrp',
                'div#content',
                'div.article_content'
            ]
            
            content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    break
            
            if content:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for unwanted in content.select('script, style, .reporter_area, .copyright'):
                    unwanted.decompose()
                
                return content.get_text(separator='\n', strip=True)
            else:
                logger.warning(f"ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
                return ''
                
        except Exception as e:
            logger.error(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return ''
    
    def crawl_naver_news(self, base_url: str, max_pages: int = 10, 
                        category: str = "economy") -> List[Dict[str, Any]]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        results = []
        seen_urls = set()
        page = 1
        
        logger.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘: {base_url}")
        
        while page <= max_pages:
            try:
                logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
                current_url = base_url.format(page)
                self.driver.get(current_url)
                time.sleep(2)
                
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                # ë„¤ì´ë²„ ë‰´ìŠ¤ ëª©ë¡ ì„ íƒìë“¤
                news_selectors = [
                    'ul.list_news > li',
                    'div.news_area > div.news_wrap',
                    'div.news_area > a',
                    'ul.content_list > li'
                ]
                
                items = []
                for selector in news_selectors:
                    items = soup.select(selector)
                    if items:
                        break
                
                if not items:
                    logger.info("âœ… ë” ì´ìƒ í•­ëª© ì—†ìŒ. ì¢…ë£Œ.")
                    break
                
                page_results = []
                for item in tqdm(items, desc=f"Page {page}"):
                    try:
                        # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                        title_tag = item.select_one('a') or item.select_one('strong.title') or item.select_one('.news_tit')
                        if not title_tag:
                            continue
                        
                        href = title_tag.get('href', '')
                        if not href:
                            continue
                        
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if href.startswith('/'):
                            full_url = "https://news.naver.com" + href
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        
                        title = title_tag.get_text(strip=True)
                        
                        if full_url in seen_urls:
                            continue
                        
                        # ë³¸ë¬¸ ì¶”ì¶œ
                        content = self.extract_detail_text(full_url)
                        
                        if content:  # ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                            page_results.append({
                                "title": title,
                                "url": full_url,
                                "content": content,
                                "category": category,
                                "source": "naver",
                                "timestamp": datetime.now().isoformat()
                            })
                            seen_urls.add(full_url)
                    
                    except Exception as e:
                        logger.error(f"âŒ í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                # í˜ì´ì§€ë³„ ê²°ê³¼ ì €ì¥
                if page_results:
                    filename = f"naver_news_{category}_page{page}.csv"
                    pd.DataFrame(page_results).to_csv(filename, index=False, encoding="utf-8-sig")
                    logger.info(f"ğŸ’¾ í˜ì´ì§€ë³„ ì €ì¥: {filename} (ì´ {len(page_results)}ê°œ)")
                
                results.extend(page_results)
                page += 1
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                page += 1
                continue
        
        return results
    
    def crawl_economy_news(self, max_pages: int = 10) -> List[Dict[str, Any]]:
        """ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        base_url = "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2=258&page={}"
        return self.crawl_naver_news(base_url, max_pages, "economy")
    
    def crawl_stock_news(self, max_pages: int = 10) -> List[Dict[str, Any]]:
        """ì£¼ì‹ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        base_url = "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2=259&page={}"
        return self.crawl_naver_news(base_url, max_pages, "stock")
    
    def crawl_company_news(self, max_pages: int = 10) -> List[Dict[str, Any]]:
        """ê¸°ì—… ë‰´ìŠ¤ í¬ë¡¤ë§"""
        base_url = "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2=260&page={}"
        return self.crawl_naver_news(base_url, max_pages, "company")
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            logger.info("ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì¢…ë£Œ")

# ì‚¬ìš© ì˜ˆì‹œ
def main():
    crawler = NaverNewsCrawler(headless=True)
    
    try:
        # ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
        print("ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        economy_news = crawler.crawl_economy_news(max_pages=3)
        print(f"ê²½ì œ ë‰´ìŠ¤: {len(economy_news)}ê°œ")
        
        # ì£¼ì‹ ë‰´ìŠ¤ í¬ë¡¤ë§
        print("ì£¼ì‹ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        stock_news = crawler.crawl_stock_news(max_pages=3)
        print(f"ì£¼ì‹ ë‰´ìŠ¤: {len(stock_news)}ê°œ")
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        all_news = economy_news + stock_news
        df = pd.DataFrame(all_news)
        df.to_csv("naver_news_all.csv", index=False, encoding="utf-8-sig")
        print(f"âœ… ì´ {len(all_news)}ê°œ ì €ì¥ ì™„ë£Œ: naver_news_all.csv")
        
    finally:
        crawler.close()

if __name__ == "__main__":
    main() 